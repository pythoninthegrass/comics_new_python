#!/usr/bin/env -S uv run --script

# /// script
# requires-python = ">=3.13,<3.14"
# dependencies = [
#     "bs4",
#     "reportlab",
#     "requests",
# ]
# [tool.uv]
# exclude-newer = "2025-12-31T00:00:00Z"
# ///

# pyright: reportMissingImports=false

import logging
import requests
import sqlite3
from bs4 import BeautifulSoup
from datetime import datetime
from reportlab.lib.pagesizes import letter
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib.units import inch
from reportlab.platypus import Paragraph, SimpleDocTemplate, Spacer

# Set up logging at start of program
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    filename='automation.log'
)

URL = "https://www.comicsbeat.com/"


def scrape_headlines(url):
    """
    Download a news page and extract headlines and URLs.
    """
    logging.info('Starting headline scraping')

    # Add headers to mimic a browser request
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    # Error handling for the request
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        logging.info(f'Successfully fetched page: {url}')
    except requests.exceptions.RequestException as e:
        logging.error(f"Error fetching the page: {e}")
        print(f"Error fetching the page: {e}")
        return

    soup = BeautifulSoup(response.text, 'html.parser')

    # Try different selectors to find headlines
    titlelines = soup.select('.entry-title')

    # Limit to only the first 10 headlines
    titlelines = titlelines[:10]

    # Debug: Check if we found anything
    print(f"Found {len(titlelines)} headlines")
    logging.info(f"Found {len(titlelines)} headlines")

    if not titlelines:
        logging.warning("No headlines found with .entry-title selector, trying alternative")
        print("No headlines found. Trying alternative selectors...")
        titlelines = soup.select('h3.entry-title')
        print(f"Found {len(titlelines)} with h3.entry-title")
        logging.info(f"Found {len(titlelines)} with h3.entry-title selector")

    if not titlelines:
        logging.error("No headlines found with any selector")
        print("Still nothing. Here are some article titles on the page:")
        all_titles = soup.find_all(['h1', 'h2', 'h3'], class_=True)
        for title in all_titles[:5]:
            print(f"  {title.get('class')} -> {title.get_text()[:50]}")
        return

    # Create/connect to SQLite database
    try:
        conn = sqlite3.connect('headlines.db')
        cursor = conn.cursor()
        logging.info('Connected to SQLite database')

        # Create table if it doesn't exist
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS headlines (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                headline TEXT NOT NULL,
                url TEXT NOT NULL,
                scraped_date TEXT NOT NULL
            )
        ''')
        logging.info('Database table ready')

        # Print headlines and save to database
        headlines_saved = 0
        for titleline in titlelines:
            link = titleline.find('a')
            if link:
                headline = link.get_text().strip()
                url = link.get('href')
                print(f"{headline}\n  â†’ {url}\n")

                # Insert into database
                cursor.execute('''
                    INSERT INTO headlines (headline, url, scraped_date)
                    VALUES (?, ?, ?)
                ''', (headline, url, datetime.now().strftime('%Y-%m-%d %H:%M:%S')))
                headlines_saved += 1

        # Commit changes and close connection
        conn.commit()
        conn.close()

        logging.info(f'Saved {headlines_saved} headlines to database')
        print(f"Saved {headlines_saved} headlines to database!")

    except sqlite3.Error as e:
        logging.error(f'Database error: {e}')
        print(f"Database error: {e}")


def generate_pdf():
    """Generate a PDF report from the headlines database"""
    logging.info('Starting PDF generation')

    # Connect to database
    try:
        conn = sqlite3.connect('headlines.db')
        cursor = conn.cursor()

        cursor.execute('SELECT headline, url, scraped_date FROM headlines ORDER BY scraped_date DESC')
        rows = cursor.fetchall()
        conn.close()

        logging.info(f'Retrieved {len(rows)} headlines from database')

    except sqlite3.Error as e:
        logging.error(f'Database error during PDF generation: {e}')
        print(f"Database error: {e}")
        return

    if not rows:
        logging.warning("No headlines in database to generate PDF")
        print("No headlines in database to generate PDF!")
        return

    try:
        # Create PDF with timestamp
        pdf_filename = f'headlines_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.pdf'
        doc = SimpleDocTemplate(pdf_filename, pagesize=letter,
                            topMargin=0.75*inch, bottomMargin=0.75*inch)
        story = []

        # Get styles
        styles = getSampleStyleSheet()
        title_style = styles['Heading1']
        headline_style = styles['Heading2']
        normal_style = styles['Normal']

        # Add title
        story.append(Paragraph("Comics Beat Headlines Report", title_style))
        story.append(Spacer(1, 0.2*inch))

        # Add metadata
        story.append(Paragraph(f"<b>Total Headlines:</b> {len(rows)}", normal_style))
        story.append(Paragraph(f"<b>Generated:</b> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", normal_style))
        story.append(Spacer(1, 0.5*inch))

        # Add each headline
        for idx, (headline, url, date) in enumerate(rows, 1):
            # Number and Headline
            story.append(Paragraph(f"<b>{idx}. {headline}</b>", headline_style))
            story.append(Spacer(1, 0.1*inch))

            # URL (clickable link)
            story.append(Paragraph(f"<b>URL:</b> <link href='{url}' color='blue'>{url}</link>", normal_style))
            story.append(Spacer(1, 0.05*inch))

            # Scraped date
            story.append(Paragraph(f"<b>Scraped:</b> {date}", normal_style))
            story.append(Spacer(1, 0.3*inch))

        # Build PDF
        doc.build(story)

        logging.info(f'PDF generated successfully: {pdf_filename}')
        print("\nâœ… PDF generated successfully!")
        print(f"ðŸ“„ Filename: {pdf_filename}")
        print(f"ðŸ“Š Total headlines: {len(rows)}")

    except Exception as e:
        logging.error(f'Error generating PDF: {e}')
        print(f"Error generating PDF: {e}")


def main():
    logging.info('='*50)
    logging.info('Starting automation script')
    logging.info('='*50)

    news_url = URL
    scrape_headlines(news_url)
    generate_pdf()

    logging.info('Automation script completed')
    logging.info('='*50)


if __name__ == "__main__":
    main()
